{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas a serem utilizadas para o desafio de hoje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dos recursos que serão necessários para cada uma das etapas até a lematização completa dos textos do desafio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('wordnet', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/home/codespace/nltk_data')\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando o Tokenizador e o Lematizador, ambos do NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 1\n",
    "\n",
    "Para cada palavra na lista abaixo, identifique seu lema (forma base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras pré-definidas e lematizadas: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'better',\n",
       " 'study',\n",
       " 'wolves',\n",
       " 'mice',\n",
       " 'children',\n",
       " 'be',\n",
       " 'eat',\n",
       " 'swim',\n",
       " 'party',\n",
       " 'leave',\n",
       " 'knives',\n",
       " 'happier',\n",
       " 'study',\n",
       " 'play',\n",
       " 'go',\n",
       " 'drive',\n",
       " 'talk']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras = [\"running\",\"better\",\"studies\",\"wolves\",\"mice\",\"children\",\"was\",\"ate\",\"swimming\",\"parties\",\"leaves\",\"knives\",\"happier\",\"studying\",\"played\",\"goes\",\"driving\",\"talked\"]\n",
    "palavras_lematizadas = [lemmatizer.lemmatize(token, pos='v') for token in palavras] # pos='v' declara que queremos lematizar como verbo\n",
    "print('Palavras pré-definidas e lematizadas: ')\n",
    "palavras_lematizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 2\n",
    "\n",
    "Para cada frase, identifique os lemas de todas as palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The children were playing in the leaves yesterday.',\n",
       " 'She studies computer science and is taking three courses.',\n",
       " 'The wolves howled at the moon while mice scurried in the grass.',\n",
       " 'He was driving faster than the cars around him.',\n",
       " 'The chefs used sharp knives to prepare the tastiest dishes.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "frases = [\"The children were playing in the leaves yesterday.\",\"She studies computer science and is taking three courses.\",\"The wolves howled at the moon while mice scurried in the grass.\",\"He was driving faster than the cars around him.\",\"The chefs used sharp knives to prepare the tastiest dishes.\"]\n",
    "frases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o loop para iterar entre a lista de frases e fazer cada um dos passos até a lematização completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The children were playing in the leaves yesterday.\n",
      "Frase  1  tokenizada: \n",
      "\n",
      "Frase  1  stopwords: \n",
      "\n",
      "Frase  1  sem stopwords: \n",
      "\n",
      "Frase  1  lematizada: \n",
      "She studies computer science and is taking three courses.\n",
      "Frase  2  tokenizada: \n",
      "\n",
      "Frase  2  stopwords: \n",
      "\n",
      "Frase  2  sem stopwords: \n",
      "\n",
      "Frase  2  lematizada: \n",
      "The wolves howled at the moon while mice scurried in the grass.\n",
      "Frase  3  tokenizada: \n",
      "\n",
      "Frase  3  stopwords: \n",
      "\n",
      "Frase  3  sem stopwords: \n",
      "\n",
      "Frase  3  lematizada: \n",
      "He was driving faster than the cars around him.\n",
      "Frase  4  tokenizada: \n",
      "\n",
      "Frase  4  stopwords: \n",
      "\n",
      "Frase  4  sem stopwords: \n",
      "\n",
      "Frase  4  lematizada: \n",
      "The chefs used sharp knives to prepare the tastiest dishes.\n",
      "Frase  5  tokenizada: \n",
      "\n",
      "Frase  5  stopwords: \n",
      "\n",
      "Frase  5  sem stopwords: \n",
      "\n",
      "Frase  5  lematizada: \n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "indice = 0\n",
    "for frase in frases:\n",
    "    print(frase)    \n",
    "    frase_tokenizada = tokenizer.tokenize(frase)\n",
    "    print('Frase ',indice + 1, ' tokenizada: ')\n",
    "    frase_tokenizada\n",
    "\n",
    "    print('')\n",
    "    frase_stopwords = [word for word in frase_tokenizada if word in stop_words]\n",
    "    print('Frase ',indice + 1, ' stopwords: ')\n",
    "    frase_stopwords\n",
    "\n",
    "    print('')\n",
    "    frase_sem_stopwords = [word for word in frase_tokenizada if word not in stop_words]\n",
    "    print('Frase ',indice + 1, ' sem stopwords: ')\n",
    "    frase_sem_stopwords\n",
    "\n",
    "    print('')\n",
    "    frase_lematizada = [lemmatizer.lemmatize(token, pos='v') for token in frase_sem_stopwords]\n",
    "    print('Frase ', indice + 1, ' lematizada: ')\n",
    "    frase_lematizada\n",
    "\n",
    "    indice = indice + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
