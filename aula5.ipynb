{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Bonjour, comment ca va ?     ',\n",
       " '    Heyyyyy, how are you doing ?   ',\n",
       " '        Hallo, wie gehts ?     ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    '   Bonjour, comment ca va ?     ',\n",
    "    '    Heyyyyy, how are you doing ?   ',\n",
    "    '        Hallo, wie gehts ?     '\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour, comment ca va ?',\n",
       " 'Heyyyyy, how are you doing ?',\n",
       " 'Hallo, wie gehts ?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text.strip() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcd Who is abcd ? That's not a real name!!! abcd\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"abcd Who is abcd ? That's not a real name!!! abcd\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Who is abcd ? That's not a real name!!! \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.strip('bdac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love koalas, koalas are the cutest animals on Earth.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love koalas, koalas are the cutest animals on Earth.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love pandas, pandas are the cutest animals on Earth.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"koala\", \"panda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"linkin park / metallica /red hot chili peppers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linkin park ', ' metallica ', 'red hot chili peppers']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love football so much. football is my passion. who else loves football ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like  minutes, this is ridiculous'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = ''.join(char for char in text if not char.isdigit())\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea OMG so tasty channel XOXO   '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"   I LOVE Pizza 999 @^_^\",\n",
    "    \"  Le Wagon is amazing, take care - 666\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love pizza', 'le wagon is amazing take care']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = [basic_cleaning(sentence) for sentence in sentences]\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Le Wagon!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"<head><body>Hello Le Wagon!</body></head>\"\"\"\n",
    "cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "\n",
    "print (cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['darkvador@gmail.com', 'batman@outlook.com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = '''\n",
    "    This is a random text, authored by darkvador@gmail.com\n",
    "    and batman@outlook.com, WOW!\n",
    "'''\n",
    "\n",
    "re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is during our darkest moments that we must focus to see the light'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'during', 'our', 'darkest', 'moments', 'that', 'we', 'must', 'focus', 'to', 'see', 'the', 'light']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure the punkt package is properly downloaded and configured\n",
    "nltk.download('punkt', quiet=False)\n",
    "\n",
    "# Set NLTK to use a specific language model - try PY3 as it might be the Python 3 compatible version\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "# Explicitly try to use a specific model\n",
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "# Alternative approach that doesn't rely on the punkt model\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "word_tokens = tokenizer.tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'is',\n",
       " 'during',\n",
       " 'our',\n",
       " 'darkest',\n",
       " 'moments',\n",
       " 'that',\n",
       " 'we',\n",
       " 'must',\n",
       " 'focus',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ей', 'этом', 'у', 'свою', 'она', 'ним', 'его', 'тоже', 'сейчас', 'нее']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'а',\n",
       " 'без',\n",
       " 'более',\n",
       " 'больше',\n",
       " 'будет',\n",
       " 'будто',\n",
       " 'бы',\n",
       " 'был',\n",
       " 'была',\n",
       " 'были',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'в',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'вдруг',\n",
       " 'ведь',\n",
       " 'во',\n",
       " 'вот',\n",
       " 'впрочем',\n",
       " 'все',\n",
       " 'всегда',\n",
       " 'всего',\n",
       " 'всех',\n",
       " 'всю',\n",
       " 'вы',\n",
       " 'где',\n",
       " 'да',\n",
       " 'даже',\n",
       " 'два',\n",
       " 'для',\n",
       " 'до',\n",
       " 'другой',\n",
       " 'его',\n",
       " 'ее',\n",
       " 'ей',\n",
       " 'ему',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'еще',\n",
       " 'ж',\n",
       " 'же',\n",
       " 'за',\n",
       " 'зачем',\n",
       " 'здесь',\n",
       " 'и',\n",
       " 'из',\n",
       " 'или',\n",
       " 'им',\n",
       " 'иногда',\n",
       " 'их',\n",
       " 'к',\n",
       " 'как',\n",
       " 'какая',\n",
       " 'какой',\n",
       " 'когда',\n",
       " 'конечно',\n",
       " 'кто',\n",
       " 'куда',\n",
       " 'ли',\n",
       " 'лучше',\n",
       " 'между',\n",
       " 'меня',\n",
       " 'мне',\n",
       " 'много',\n",
       " 'может',\n",
       " 'можно',\n",
       " 'мой',\n",
       " 'моя',\n",
       " 'мы',\n",
       " 'на',\n",
       " 'над',\n",
       " 'надо',\n",
       " 'наконец',\n",
       " 'нас',\n",
       " 'не',\n",
       " 'него',\n",
       " 'нее',\n",
       " 'ней',\n",
       " 'нельзя',\n",
       " 'нет',\n",
       " 'ни',\n",
       " 'нибудь',\n",
       " 'никогда',\n",
       " 'ним',\n",
       " 'них',\n",
       " 'ничего',\n",
       " 'но',\n",
       " 'ну',\n",
       " 'о',\n",
       " 'об',\n",
       " 'один',\n",
       " 'он',\n",
       " 'она',\n",
       " 'они',\n",
       " 'опять',\n",
       " 'от',\n",
       " 'перед',\n",
       " 'по',\n",
       " 'под',\n",
       " 'после',\n",
       " 'потом',\n",
       " 'потому',\n",
       " 'почти',\n",
       " 'при',\n",
       " 'про',\n",
       " 'раз',\n",
       " 'разве',\n",
       " 'с',\n",
       " 'сам',\n",
       " 'свою',\n",
       " 'себе',\n",
       " 'себя',\n",
       " 'сейчас',\n",
       " 'со',\n",
       " 'совсем',\n",
       " 'так',\n",
       " 'такой',\n",
       " 'там',\n",
       " 'тебя',\n",
       " 'тем',\n",
       " 'теперь',\n",
       " 'то',\n",
       " 'тогда',\n",
       " 'того',\n",
       " 'тоже',\n",
       " 'только',\n",
       " 'том',\n",
       " 'тот',\n",
       " 'три',\n",
       " 'тут',\n",
       " 'ты',\n",
       " 'у',\n",
       " 'уж',\n",
       " 'уже',\n",
       " 'хорошо',\n",
       " 'хоть',\n",
       " 'чего',\n",
       " 'чем',\n",
       " 'через',\n",
       " 'что',\n",
       " 'чтоб',\n",
       " 'чтобы',\n",
       " 'чуть',\n",
       " 'эти',\n",
       " 'этого',\n",
       " 'этой',\n",
       " 'этом',\n",
       " 'этот',\n",
       " 'эту',\n",
       " 'я'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Baixar o recurso de stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Agora tente carregar as stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Exibir algumas stopwords\n",
    "print(list(stop_words)[:10])\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos necessários do NLTK\n",
    "nltk.download('stopwords', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('wordnet', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/home/codespace/nltk_data')  # Open Multilingual WordNet\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "def basic_cleaning(text):\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontuações\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza um texto dividindo por espaços\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords da lista de tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lematiza uma lista de tokens usando WordNetLemmatizer do NLTK\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após limpeza básica: he was running and eating at the same time he has a bad habit of swimming after playing hours in the sun\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemplo de uso\n",
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'\n",
    "\n",
    "# Etapa 1: Limpeza básica\n",
    "cleaned_sentence = basic_cleaning(sentence)\n",
    "print(\"Após limpeza básica:\", cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', 'he', 'has', 'a', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'hours', 'in', 'the', 'sun']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(cleaned_sentence)\n",
    "print(\"Após tokenização:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após remoção de stopwords: ['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hours', 'sun']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após lematização: ['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hour', 'sun']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
